# Lec 4: Scaling Laws and Emergent Behavior - Tatsu Hashimoto 

attendance: 
https://forms.gle/Qow74t8tK4UbMyqV8
https://forms.gle/CgdW2kxRgBFRpScr9

## Summary
- 2 sentence summary
  - 
  - 

Abstract:

Paper: 

My summary of the abstract:

FMs summary:

Copypaste summary:

- main points:
  - part 1: take aways Scaling Laws:
    - todo
  - part 2: 
    - todo

refs:
  - canvas: 


## Notes

- misc info:
  - data composition affects offset but not the slope.  
  - how do these scaling laws predict "emergence" e.g. when there is a jump. How do we know lstms woulnd't have had a jump? (and be better)
  - small difference using scaling laws can lead to big difference (caution)
  - big models tend to converge quickly at first, taken advantage of this

# Question

Q1: Definition of scaling laws.
A1: 

Q2: why polyomial?
A2: Tatsu's guess, it's very hard to break polynmial decay e.g. active learning can get exponential
improvements. 

Q3: data composition affects offset but not the slope. 
A3: it's mainly not the data distribution but the model.

Q4: can you give an intuition for why data composition only affects offset? Also, can you define data composition? Thanks in advance! 
can you give an intuition for why data composition only affects offset? Also, can you define data composition? 
Thanks in advance!  (@michaelzhang tagging a TA couldn't find Professor Hashimoto in discord)
A4: 

Q5: aren't statistical learning bounds vacuous? Why are we drawing intuitions from this? 
I thought they didn't apply in practice. Feel free to correct me :)
A5: 

Q6: Can scaling laws predict "emergence" e.g. when there is a jump? How do we know lstms woulnd't have had a jump? (and be better)
A6: 

Q7: Emergence vs large scale learning.
A7:

Q8: what is this word in context task?
A8:

Q9: Grokking vs Emergence
A9: 

Q10: when do large language models stop improving?
A10:

Q11: @ChrisRe#7896 Related to the question of context length. The discussion with @Rylan#3780 if context length or batch size helps the transformer model more -- the question inspired from the common comments that more tokens is better. Is there a reason you decided to focus on larger context length for FMs? Is the "more token consumption the reason" or some other reason? Thanks!
A11:

Q12: Is the solpe affect more by the model, learning algrithm than the data?
A12:
(if this is true this would motivate "non brute force research" e.g. what Tatsu calls increasing just data size etc. 
since it is the exponent <-> slope in log scale is the exponent of polynomial on linear/normal scale)